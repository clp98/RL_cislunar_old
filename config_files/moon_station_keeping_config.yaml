run: ppo  #algorithm used
stop:
    training_iterations: 2500  #maximum number of gym iterations

custom_metrics:  #list of problem-specific metrics one wants to track during training through tensorboard
   - dist_r
   - dist_v
   
postproc_data:
    episode_step_data:  #data we want after every step
       - x
       - y
       - z
       - vx
       - vy
       - vz
       - m
       - t
       - Fx
       - Fy
       - Fz
       - F_mod
       - dist_r
       - dist_v
    

num_eval_episodes: 1
eval_env_config:
    env_config:
        prng_seed: 0

config:  #all the data necessary (it permits to work in parallel)
    num_workers: 10
    num_envs_per_worker: 6
    num_gpus: 0
    num_cpus_per_worker: 0.6
    num_gpus_per_worker: 0
    num_cpus_for_driver: 2
    rollout_fragment_length: 100
    batch_mode: complete_episodes  #complete_episodes or troncate_episodes (complete --> it stops the episodes when done=True)
    train_batch_size: 30000  #step complessivi di tutti i worker di tutti gli environment
    model:  #definisce la rete neurale
        fcnet_hiddens: [70,46,30]  #numero di layer (2) e numero di neuroni per layer (64)
        fcnet_activation: tanh  #activation function used
        use_lstm: False
        vf_share_layers: False  #False-->rete separata
        free_log_std: True
    gamma: 1.
    log_level: INFO
    framework: tf
    explore: True
    ignore_worker_failures: True
    evaluation_parallel_to_training: True
    evaluation_interval: 1
    evaluation_duration: 1
    evaluation_duration_unit: episodes
    evaluation_config:
        explore: False
        env_config:
            eps_schedule: [[0, 1.0e-03], [1, 1.0e-03]]
    evaluation_num_workers: 0
    always_attach_evaluation_results: True
    use_critic: True
    use_gae: True
    lambda: 1.
    kl_coeff: 0.
    sgd_minibatch_size: 3000
    shuffle_sequences: True
    num_sgd_iter: 50
    lr: 10.e-4  #learning rate of the network
    lr_schedule: [[0, 1.0e-04], [60000000, 1.0e-06]]
    vf_loss_coeff: 0.5
    clip_param: 0.05  #by how much (percentage) the policy can vary
    vf_clip_param: 1.
    env: environment.moon_station_keeping_env.Moon_Station_Keeping 
    callbacks:
        epsConstraintCallbacks



    env_config:  
        filename: /home/carlo/RL_cislunar/Halo_L1_rv.txt
        eps_schedule: [[0,1],[2500,1.0e-03]]  #2500 is equal to training_iterations
        num_steps: 100  #number of iterations
        Isp: 3000 #specific impulse
        tf: 1000  #maximum duration of the mission
        Fmax: 0.04  #maximum thrust of the engine
        m_sc: 1  #initial mass of the spacecraft
        w: 0.2  #weight of the mass reward
        single_matrix: True