# #Finish or start training from a certain checkpoint
# load:
#     trainer_dir: "results/PPO_2023-07-03_11-46-17/"
#     prev_exp_dirs:
#         - "results/PPO_2023-07-03_11-46-17/PPO_environment.Moon_Station_Keeping_Env.Moon_Station_Keeping_Env_e5286_00000_0_2023-07-03_11-46-18/"
#     prev_last_cps:
#         - 493

run: ppo  #algorithm used
stop:  #criterias to stop the training
    training_iteration: 5000  #maximum number of gym iterations

custom_metrics:  #list of problem-specific metrics one wants to track during training through tensorboard
   - dist_r_mean
   - dist_v_mean
   - mf
   - epsilon
   - failure
   
postproc_data:
    episode_step_data:  #data we want after every step
       - x 
       - y 
       - z
       - vx
       - vy
       - vz
       - x_Halo
       - y_Halo
       - z_Halo
       - vx_Halo
       - vy_Halo
       - vz_Halo
       - m
       - t
       - Fx
       - Fy
       - Fz
       - F_mod
       - dist_r
       - dist_v
    

num_eval_episodes: 1  #number od episodes to evaluate 
# eval_env_config:
#     env_config:
#         prng_seed: 0  #seed of random numbers

config:  #all the data necessary requested by Ray library (driver = cio che fa l'update della rete)

    #cpu/gpu combination
    #VENUS
    num_workers: 10  #number of copies of the network
    num_envs_per_worker: 10  #number of environments
    num_gpus: 0  #number of gpu
    num_cpus_per_worker: 1  #number of cpu
    num_gpus_per_worker: 0  #number of gpu
    num_cpus_for_driver: 1  #number of cpu per driver

    rollout_fragment_length: 100
    batch_mode: complete_episodes  #complete_episodes or troncate_episodes (complete --> it stops the episodes when done=True)
    train_batch_size: 10000  #step complessivi di tutti i worker di tutti gli environment
    model:  #definisce la rete neurale

        #MLP
        fcnet_hiddens: [85,41,20]  #numero di layer, numero di neuroni per layer 
        fcnet_activation: tanh  #activation function used
        use_lstm: False
        vf_share_layers: False  #False --> rete separata (vf=value function)
        free_log_std: True  #True --> valori separati dalla rete

        # #LSTM
        # fcnet_hiddens: [50,35]  #numero di layer, numero di neuroni per layer 
        # fcnet_activation: tanh  #activation function used
        # use_lstm: True
        # max_seq_len: 45  #how many steps back the recurring network looks (it keeps memory for max_seq_len steps), provare minore (10) [25]
        # lstm_cell_size: 20  #dimension of the recurring network, provare minore (10) [20]
        # lstm_use_prev_action: True
        # lstm_use_prev_reward: False
        # vf_share_layers: True  #False --> rete separata (vf=value function)
        # free_log_std: False  #True --> valori separati dalla rete

    gamma: 0.999  #discount factor
    log_level: INFO  #che info voglio sia stampata a schermo
    framework: tf  #framework usato
    explore: True  #esplorazione durante l'allenamento
    ignore_worker_failures: True  #ignora i worker che falliscono e non ferma l'allenamento
    evaluation_parallel_to_training: True  #valutazione in parallelo
    evaluation_interval: 1  #intervallo di valutazione
    evaluation_num_episodes: 5  #durata della valutazione
    evaluation_config:  #config della valutazione
        explore: False  #esplorazione
        env_config:
            eps_schedule: [[0, 0.1], [1, 0.1]]  #epsilon law parameters
    evaluation_num_workers: 5  #number of workers per valutazione
    use_critic: True
    use_gae: True
    lambda: 0.98  #discount factor (=[0,1])
    kl_coeff: 0.
    sgd_minibatch_size: 1000  #grandezza batch per gradient descent
    shuffle_sequences: True
    num_sgd_iter: 30  #quante iterazioni di gradient descent
    lr: 1.0e-04  #learning rate of the network = alpha
    #lr_schedule: [[0, 1.0e-04], [60000000, 1.0e-05]]  #decreasing learning rate ([][trainbatch size*training iterations,]))
    vf_loss_coeff: 0.5
    clip_param: 0.05  #by how much (percentage) the policy can vary
    vf_clip_param: 10.
    env: environment.Moon_Station_Keeping_Env.Moon_Station_Keeping_Env  #which is the environment
    callbacks:
        epsConstraintCallbacks



    env_config:  #dictionary with all values that represent the environment
        
        #sc physicial quantities
        Isp: 3000  #specific impulse [s]
        Fmax: 4.0e-02  #maximum thrust of the engine [], Fmax[N]=(value)*2.7307388899605793e-06*spacecraft mass
        m_sc: 1.  #initial mass of the spacecraft
        A_sc: 0.1  #frontal area of the sc

        #important quantities
        eps_schedule: [[0,1.0],[5000,0.1]]  #([starting step,initial epsilon value],[training iterations,final epsilon value]))
        num_steps: 100  #number of iterations
        max_dist_r: 500  #maximum dist_r allowed
        num_Halos: 5  #number of Halos to extract
        w_m: 0.02  #weight of the mass reward part
        w_r: 1.  #weight of the distance reward part
        tf: 2.7430009  #maximum duration of the mission [t_star], NB 2.78753463 is the maximum period of all orbits
        dr_max: 20.  #maximum position error
        dv_max: 0.001  #maximum velocity error
        epsilon_r_scale: 200  #scaling to have the right position measure (200 km --> 20 km)
        epsilon_v_scale: 3  #scaling to have the right velocity measure (2 m/s --> 0.2 m/s)

        #flags
        filename: PO_files/l1_halo_north.txt  #file to extract Halo reference data from
        threebody: False  #3-body dynamics or 4-body dynamics
        error_initial_position: True  #error on spacecraft initial position (with max values dr_max and dv_max )
        show_dist_from_Halo: False
        wild_reward: False
        dist_r_method: 3  #1: dist_r lungo la Halo, 2: dist_r ortogonale alla Halo, 3: dist_r corretto

        
        