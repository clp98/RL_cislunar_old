# #Finish or start training from a certain checkpoint
# load:
#     trainer_dir: "results/PPO_2023-05-31_09-10-41/"  
#     prev_exp_dirs:
#         - "results/PPO_2023-05-31_09-10-41/PPO_environment.Moon_Station_Keeping_Env.Moon_Station_Keeping_Env_b0626_00000_0_2023-05-31_09-10-41/"  #
        
#     prev_last_cps:
#         - 5000


run: ppo  #algorithm used
stop:  #criterias to stop the training
    training_iteration: 6000  #maximum number of gym iterations

custom_metrics:  #list of problem-specific metrics one wants to track during training through tensorboard
   - dist_r_mean
   - dist_v_mean
   - mf
   - epsilon
   - failure

postproc_data:
    episode_step_data:  #data we want after every step
       - x # #Finish or start training from a certain checkpoint
# load:
#     trainer_dir: "results/PPO_2023-05-31_09-10-41/"  
#     prev_exp_dirs:
#         - "results/PPO_2023-05-31_09-10-41/PPO_environment.Moon_Station_Keeping_Env.Moon_Station_Keeping_Env_b0626_00000_0_2023-05-31_09-10-41/"  #
        
#     prev_last_cps:
#         - 5000


run: ppo  #algorithm used
stop:  #criterias to stop the training
    training_iteration: 6000  #maximum number of gym iterations

custom_metrics:  #list of problem-specific metrics one wants to track during training through tensorboard
   - dist_r_mean
   - dist_v_mean
   - mf
   - epsilon
   - failure

postproc_data:
    episode_step_data:  #data we want after every step
       - x 
       - y 
       - z
       - vx
       - vy
       - vz
       - m
       - t
       - Fx#prova buona - riallenamento di (results/PPO_2023-05-30_10-07-25) per ultimi 800 step

config:  #all the data necessary requested by Ray library (driver = cio che fa l'update della rete)
    num_workers: 25  #number of copies of the network
    num_envs_per_worker: 4  #number of environments
    num_gpus: 1  #number of gpu
    num_cpus_per_worker: 0.64  #number of cpu
    num_gpus_per_worker: 0  #number of gpu
    num_cpus_for_driver: 0  #number of cpu per driver
    rollout_fragment_length: 100
    batch_mode: complete_episodes  #complete_episodes or troncate_episodes (complete --> it stops the episodes when done=True)
    train_batch_size: 10000  #step complessivi di tutti i worker di tutti gli environment
    model:  #definisce la rete neurale

        # #MLP
        # fcnet_hiddens: [50,35,20]  #numero di layer, numero di neuroni per layer 
        # fcnet_activation: tanh  #activation function used
        # use_lstm: False
        # vf_share_layers: False  #False --> rete separata (vf=value function)
        # free_log_std: True  #True --> valori separati dalla rete

        #LSTM
        fcnet_hiddens: [50,35]  #numero di layer, numero di neuroni per layer 
        fcnet_activation: tanh  #activation function used
        use_lstm: True
        max_seq_len: 25
        lstm_cell_size: 20
        lstm_use_prev_action: True
        lstm_use_prev_reward: False
        vf_share_layers: True  #False --> rete separata (vf=value function)
        free_log_std: False  #True --> valori separati dalla rete

    gamma: 0.999  #discount factor
    log_level: INFO  #che info voglio sia stampata a schermo
    framework: tf  #framework usato
    explore: True  #esplorazione durante l'allenamento
    ignore_worker_failures: True  #ignora i worker che falliscono e non ferma l'allenamento
    evaluation_parallel_to_training: True  #valutazione in parallelo
    evaluation_interval: 1  #intervallo di valutazione
    evaluation_num_episodes: 25  #durata della valutazione
    evaluation_config:  #config della valutazione
        explore: False  #esplorazione
        env_config:
            eps_schedule: [[0, 1.0e-03], [1, 1.0e-03]]  #epsilon law parameters
    evaluation_num_workers: 25  #dist_v_mean
    use_critic: True
    use_gae: True
    lambda: 0.98  #discount factor (=[0,1])
    kl_coeff: 0.
    sgd_minibatch_size: 1000  #grandezza batch per gradient descent
    shuffle_sequences: True
    num_sgd_iter: 30  #quante iterazioni di gradient descent
    lr: 1.0e-04  #learning rate of the network = alpha
    #lr_schedule: [[0, 1.0e-04], [60000000, 1.0e-05]]  #decreasing learning rate ([][trainbatch size*training iterations,]))
    vf_loss_coeff: 0.5
    clip_param: 0.05  #by how much (percentage) the policy can vary
    vf_clip_param: 10.
    env: environment.Moon_Station_Keeping_Env.Moon_Station_Keeping_Env  #which is the environment
    callbacks:
        epsConstraintCallbacks



    env_config:  #dictionary with all values that represent the environment
        filename: ./Halo_L1_rv.txt #PO_files/l1_halo_north.txt
        eps_schedule: [[0,0.001],[6000,0.001]]  #([][training iterations,]))
        num_steps: 100  #number of iterations
        Isp: 3000  #specific impulse [s]
        tf: 2.74291184  #maximum duration of the mission [t_star]
        Fmax: 4.0e-02  #maximum thrust of the engine [], Fmax[N]=(value)*2.7307388899605793e-06*spacecraft mass
        m_sc: 1.  #initial mass of the spacecraft
        A_sc: 0.1  #frontal area of the sc
        w: 0.02  #weight of the mass reward part
        w_r: 1.  #weight of the distance reward part
        threebody: True  #3-body dynamics or 4-body dynamics

        single_matrix: True
        show_dist_from_Halo: False
        wild_reward: False
        max_dist_r: -1  #maximum dist_r allowed
        num_Halos: 1  #number of Halos to extract
        error_initial_position: True  #error on spacecraft initial position
        dist_r_method: 3  #1: dist-r lungo la Halo, 2: dist-r ortogonale alla Halo
        dr_max: 20.
        dv_max: 0.001
        epsilon_r_scale: 1.0e+04
        epsilon_v_scale: 0.1
        
       - y 
       - z
       - vx
       - vy
       - vz
       - m
       - t
       - Fx#prova buona - riallenamento di (results/PPO_2023-05-30_10-07-25) per ultimi 800 step

config:  #all the data necessary requested by Ray library (driver = cio che fa l'update della rete)
    num_workers: 25  #number of copies of the network
    num_envs_per_worker: 4  #number of environments
    num_gpus: 1  #number of gpu
    num_cpus_per_worker: 0.64  #number of cpu
    num_gpus_per_worker: 0  #number of gpu
    num_cpus_for_driver: 0  #number of cpu per driver
    rollout_fragment_length: 100
    batch_mode: complete_episodes  #complete_episodes or troncate_episodes (complete --> it stops the episodes when done=True)
    train_batch_size: 10000  #step complessivi di tutti i worker di tutti gli environment
    model:  #definisce la rete neurale

        # #MLP
        # fcnet_hiddens: [50,35,20]  #numero di layer, numero di neuroni per layer 
        # fcnet_activation: tanh  #activation function used
        # use_lstm: False
        # vf_share_layers: False  #False --> rete separata (vf=value function)
        # free_log_std: True  #True --> valori separati dalla rete

        #LSTM
        fcnet_hiddens: [50,35]  #numero di layer, numero di neuroni per layer 
        fcnet_activation: tanh  #activation function used
        use_lstm: True
        max_seq_len: 25
        lstm_cell_size: 20
        lstm_use_prev_action: True
        lstm_use_prev_reward: False
        vf_share_layers: True  #False --> rete separata (vf=value function)
        free_log_std: False  #True --> valori separati dalla rete

    gamma: 0.999  #discount factor
    log_level: INFO  #che info voglio sia stampata a schermo
    framework: tf  #framework usato
    explore: True  #esplorazione durante l'allenamento
    ignore_worker_failures: True  #ignora i worker che falliscono e non ferma l'allenamento
    evaluation_parallel_to_training: True  #valutazione in parallelo
    evaluation_interval: 1  #intervallo di valutazione
    evaluation_num_episodes: 25  #durata della valutazione
    evaluation_config:  #config della valutazione
        explore: False  #esplorazione
        env_config:
            eps_schedule: [[0, 1.0e-03], [1, 1.0e-03]]  #epsilon law parameters
    evaluation_num_workers: 25  #dist_v_mean
    use_critic: True
    use_gae: True
    lambda: 0.98  #discount factor (=[0,1])
    kl_coeff: 0.
    sgd_minibatch_size: 1000  #grandezza batch per gradient descent
    shuffle_sequences: True
    num_sgd_iter: 30  #quante iterazioni di gradient descent
    lr: 1.0e-04  #learning rate of the network = alpha
    lr_schedule: [[0, 1.0e-04], [60000000, 1.0e-05]]  #decreasing learning rate ([][trainbatch size*training iterations,]))
    vf_loss_coeff: 0.5
    clip_param: 0.05  #by how much (percentage) the policy can vary
    vf_clip_param: 10.
    env: environment.Moon_Station_Keeping_Env.Moon_Station_Keeping_Env  #which is the environment
    callbacks:
        epsConstraintCallbacks



    env_config:  #dictionary with all values that represent the environment
        filename: ./Halo_L1_rv.txt #PO_files/l1_halo_north.txt
        eps_schedule: [[0,0.1],[6000,0.001]]  #([][training iterations,]))
        num_steps: 100  #number of iterations
        Isp: 3000  #specific impulse [s]
        tf: 2.74291184  #maximum duration of the mission [t_star]
        Fmax: 4.0e-02  #maximum thrust of the engine [], Fmax[N]=(value)*2.7307388899605793e-06*spacecraft mass
        m_sc: 1.  #initial mass of the spacecraft
        A_sc: 0.1  #frontal area of the sc
        w: 0.02  #weight of the mass reward part
        w_r: 1.  #weight of the distance reward part
        threebody: False  #3-body dynamics or 4-body dynamics

        single_matrix: True
        show_dist_from_Halo: False
        wild_reward: False
        max_dist_r: -1  #maximum dist_r allowed
        num_Halos: 1  #number of Halos to extract
        error_initial_position: True  #error on spacecraft initial position
        dist_r_method: 3  #1: dist-r lungo la Halo, 2: dist-r ortogonale alla Halo
        dr_max: 20.
        dv_max: 0.001
        epsilon_r_scale: 1.0e+04
        epsilon_v_scale: 0.1
        